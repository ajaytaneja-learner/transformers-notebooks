{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Here we compute the similarity between the query and the key vectors\n",
    "    by calculating the multiplication of Q with K (i.e. Q. K ^T).\n",
    "    We divide this by square root of the dimension of k to reduce the\n",
    "    variance. We pass the resultant through a softmax to get a matrix of\n",
    "    probabilities which are the \"Attention Scores\" denoting (numerically)\n",
    "    how much each word/token in a sentence is related to the rest of the\n",
    "    words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : tensor\n",
    "        This is a tensor of dimension batch size x number of attention heads\n",
    "        x sequence length x length of the query vector of each head.\n",
    "    k : tensor\n",
    "        This is a tensor of dimension batch size x number of attention heads\n",
    "        x sequence length x length of the key vector of each head.\n",
    "    v : tensor\n",
    "        This is a tensor of dimension batch size x number of attention heads\n",
    "        x sequence length x length of the value vector of each head.\n",
    "    mask : matrix, optional\n",
    "        In the Encoder, we do not require masking, in Decoder we do\n",
    "        require masking as we do not want to know the relavence of\n",
    "        the next words. We do not want the behaviour to be\n",
    "        bi-directional for language,\n",
    "        by default None\n",
    "  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    values and attention scores\n",
    "    type: matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    print(f\"scaled.size() : {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
    "        # Broadcasting add. So just the last N dimensions need to match\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the Multihead Attention class defined by 3 arguments:\n",
    "\n",
    "    input_dim: This represents the vector dimension of every word\n",
    "    that goes into the attention unit.\n",
    "\n",
    "    d_model: output of the attention unit for every single word\n",
    "    (i.e. after coming out as a value vector)\n",
    "\n",
    "    num_heads: number of attention heads\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \n",
    "        \"\"\"\n",
    "        This is the __init__ method the constructor of the class.\n",
    "        It carries out the follwoing actions:\n",
    "        a) Calls the superclass nn.Module constructor with\n",
    "        super().__init__().\n",
    "        b) Sets the input arguments as the attributes of the class for\n",
    "        later use (self.input_dim, self.d_model, self.num_heads).\n",
    "        c) Calculates the dimension of the attention head by dividing\n",
    "        d_model by number of heads\n",
    "\n",
    "        Further we have 2 linear layers:\n",
    "        a) self.qkv_layer: This represents the qkv_later which\n",
    "        takes the input vector and maps it to the concatenated q, k, v\n",
    "        vectors respectively.\n",
    "\n",
    "        b)self.linear_layer: This linear layer is used to process\n",
    "        the concatenated outputs of all attention heads.\n",
    "        It takes the concatenated results and maps them\n",
    "        back to the original d_model dimension.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : integer\n",
    "            As defined above\n",
    "        d_model : integer\n",
    "            As defined above\n",
    "        num_heads : integer\n",
    "           As defined above\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Here we carry out the multihead attention mechanism.\n",
    "        The multi head attention mechanism involves the follwoing steps:\n",
    "        1) We pass the vector from positional encoding through the q, k, v layer\n",
    "        2) The q, k, v layer is a linear layer which transforms vector from\n",
    "        positional encoding into a concatenated q, k, v vectors expressing\n",
    "        query, key, value as discussed in theory.\n",
    "        3) The tensor of batch size x sequence length x concatenated q, k, v\n",
    "        length (e.g. 512 x 3) is reshaped into batch size x sequence length\n",
    "        x no. of heads x head dimension.\n",
    "        4) Permute - we switch around the 2nd and 3rd dimension\n",
    "        5) Chunk - i.e. we obtain the query, key and value vector individually by breaking down\n",
    "        the entire tensor by its last dimension/\n",
    "        6) We get the value vector and the matrix of attention score through\n",
    "        the scalar_dot_product function.\n",
    "        7) We pass the value vector through another linear layer in order to\n",
    "        exchange the information through various heads.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tensor\n",
    "            tensor from positional encoding of size batch_size, sequence_length,\n",
    "            input_dim\n",
    "        mask : matrix, optional\n",
    "        In the Encoder, we do not require masking, in Decoder we do\n",
    "        require masking as we do not want to know the relavence of\n",
    "        the next words. We do not want the behaviour to be\n",
    "        bi-directional for language,\n",
    "        by default None\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            final concatenated value tensor\n",
    "        \"\"\"\n",
    "        batch_size, max_sequence_length, d_model = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
    "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    This is a class to carry out PositionwiseNormalization along the feature\n",
    "    dimension of the word embeddings. It is done to ensure that the values\n",
    "    are consistent and hence do not affect upstream process in feed forward\n",
    "    network.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        \n",
    "        \"\"\"\n",
    "        The constructor (__init__) initializes the PositionwiseNormalization\n",
    "        object with the following parameters:\n",
    "\n",
    "        parameters_shape : integer\n",
    "            This is the shape of the parameters (gamma and beta) used for\n",
    "            normalization. It specifies the dimensions over which normalization\n",
    "            will be applied.\n",
    "\n",
    "        eps : decimal\n",
    "            This is a small constant added to the denominator to prevent\n",
    "            division by zero (avoiding numerical instability).\n",
    "\n",
    "        Inside the constructor, the class initializes two learnable parameters:\n",
    "\n",
    "        self.gamma:\n",
    "            It's initialized as a learnable parameter with ones,\n",
    "            meaning it starts with no scaling (identity operation).\n",
    "\n",
    "        self.beta:\n",
    "            It's initialized as a learnable parameter with zeros,\n",
    "            meaning it starts with no shift.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        The forward function is where the actual Layer Normalization is\n",
    "        applied to the input tensor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : tensor\n",
    "            This is a tensor of word embeddings\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out   : tensor\n",
    "            This is a tensor of normalized output\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        print(f\"Mean ({mean.size()})\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(f\"Standard Deviation  ({std.size()})\")\n",
    "        y = (inputs - mean) / std\n",
    "        print(f\"y: {y.size()}\")\n",
    "        out = self.gamma * y  + self.beta\n",
    "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
    "        print(f\"out: {out.size()}\")\n",
    "        return out\n",
    "\n",
    "  \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        \n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x)\n",
    "        print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Whenever an instance of EncoderLayer is created its constructor is called. \n",
    "    In the constructor, several instances of sub-classes, \n",
    "    including Muli-head attention, Layer Normalization, Positionwise feed forward \n",
    "    network are created.\n",
    "    \n",
    "    These sub-classes are all blocks of the Encoder component of the \n",
    "    Transformer Architecture \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_x = x\n",
    "        x = self.attention(x, mask=None)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    We have a class called Encoder which is inherited/derived from the nn.module.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        the constructor __init__ method:\n",
    "        \n",
    "        1) Initializes an instance of the Encoder class and \n",
    "        takes 5 parameters – d_model, ffn_hidden, num_heads, drop_prob, num_layers as \n",
    "        explained in my blog.\n",
    "        \n",
    "        2) EncoderLayer()is used to create a single instance of the EncoderLayer class \n",
    "        which as discussed above. \n",
    "        \n",
    "        3) (*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                     for _ in range(num_layers)]) \n",
    "        is a list comprehension that creates a list containing num_layers \n",
    "        identical instances of EncoderLayer class \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
