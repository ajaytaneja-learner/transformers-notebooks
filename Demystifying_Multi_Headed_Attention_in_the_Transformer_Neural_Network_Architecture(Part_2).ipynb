{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwhU7fYj4TbC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Here we compute the similarity between the query and the key vectors\n",
        "    by calculating the multiplication of Q with K (i.e. Q. K ^T).\n",
        "    We divide this by square root of the dimension of k to reduce the\n",
        "    variance. We pass the resultant through a softmax to get a matrix of\n",
        "    probabilities which are the \"Attention Scores\" denoting (numerically)\n",
        "    how much each word/token in a sentence is related to the rest of the\n",
        "    words.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    q : tensor\n",
        "        This is a tensor of dimension batch size x number of attention heads\n",
        "        x sequence length x length of the query vector of each head.\n",
        "    k : tensor\n",
        "        This is a tensor of dimension batch size x number of attention heads\n",
        "        x sequence length x length of the key vector of each head.\n",
        "    v : tensor\n",
        "        This is a tensor of dimension batch size x number of attention heads\n",
        "        x sequence length x length of the value vector of each head.\n",
        "    mask : matrix, optional\n",
        "        In the Encoder, we do not require masking, in Decoder we do\n",
        "        require masking as we do not want to know the relavence of\n",
        "        the next words. We do not want the behaviour to be\n",
        "        bi-directional for language,\n",
        "        by default None\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    _type_\n",
        "        _description_\n",
        "    \"\"\"\n",
        "\n",
        "    d_k = q.size()[-1]\n",
        "    print(\"q.size() = \", q.size())\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n"
      ],
      "metadata": {
        "id": "oe8xhQYvaycL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    This is the Multihead Attention class defined by 3 arguments:\n",
        "\n",
        "    input_dim: This represents the vector dimension of every word\n",
        "    that goes into the attention unit.\n",
        "\n",
        "    d_model: output of the attention unit for every single word\n",
        "    (i.e. after coming out as a value vector)\n",
        "\n",
        "    num_heads: number of attention heads\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, d_model, num_heads):\n",
        "\n",
        "        \"\"\"\n",
        "        This is the __init__ method the constructor of the class.\n",
        "        It carries out the follwoing actions:\n",
        "        a) Calls the superclass nn.Module constructor with\n",
        "        super().__init__().\n",
        "        b) Sets the input arguments as the attributes of the class for\n",
        "        later use (self.input_dim, self.d_model, self.num_heads).\n",
        "        c) Calculates the dimension of the attention head by dividing\n",
        "        d_model by number of heads\n",
        "\n",
        "        Further we have 2 linear layers:\n",
        "        a) self.qkv_layer: This represents the qkv_later which\n",
        "        takes the input vector and maps it to the concatenated q, k, v\n",
        "        vectors respectively.\n",
        "\n",
        "        b)self.linear_layer: This linear layer is used to process\n",
        "        the concatenated outputs of all attention heads.\n",
        "        It takes the concatenated results and maps them\n",
        "        back to the original d_model dimension.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : integer\n",
        "            As defined above\n",
        "        d_model : integer\n",
        "            As defined above\n",
        "        num_heads : integer\n",
        "           As defined above\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Here we carry out the multihead attention mechanism.\n",
        "        The multi head attention mechanism involves the follwoing steps:\n",
        "        1) We pass the vector from positional encoding through the q, k, v layer\n",
        "        2) The q, k, v layer is a linear layer which transforms vector from\n",
        "        positional encoding into a concatenated q, k, v vectors expressing\n",
        "        query, key, value as discussed in theory.\n",
        "        3) The tensor of batch size x sequence length x concatenated q, k, v\n",
        "        length (e.g. 512 x 3) is reshaped into batch size x sequence length\n",
        "        x no. of heads x head dimension.\n",
        "        4) Permute - we switch around the 2nd and 3rd dimension\n",
        "        5) Chunk - i.e. we obtain the query, key and value vector individually by breaking down\n",
        "        the entire tensor by its last dimension/\n",
        "        6) We get the value vector and the matrix of attention score through\n",
        "        the scalar_dot_product function.\n",
        "        7) We pass the value vector through another linear layer in order to\n",
        "        exchange the information through various heads.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : tensor\n",
        "            tensor from positional encoding of size batch_size, sequence_length,\n",
        "            input_dim\n",
        "        mask : matrix, optional\n",
        "        In the Encoder, we do not require masking, in Decoder we do\n",
        "        require masking as we do not want to know the relavence of\n",
        "        the next words. We do not want the behaviour to be\n",
        "        bi-directional for language,\n",
        "        by default None\n",
        "\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tensor\n",
        "            final concatenated value tensor\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "hgEqkgD6arJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 512\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "batch_size = 1\n",
        "sequence_length = 4\n",
        "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
        "\n",
        "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
        "out = model.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "989TxkbEawkw",
        "outputId": "e07c74aa-7fd2-42fc-eef1-e23428cfd30d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.size(): torch.Size([1, 4, 512])\n",
            "qkv.size(): torch.Size([1, 4, 1536])\n",
            "qkv.size(): torch.Size([1, 4, 8, 192])\n",
            "qkv.size(): torch.Size([1, 8, 4, 192])\n",
            "q size: torch.Size([1, 8, 4, 64]), k size: torch.Size([1, 8, 4, 64]), v size: torch.Size([1, 8, 4, 64]), \n",
            "q.size() =  torch.Size([1, 8, 4, 64])\n",
            "values.size(): torch.Size([1, 8, 4, 64]), attention.size:torch.Size([1, 8, 4, 4]) \n",
            "values.size(): torch.Size([1, 4, 512])\n",
            "out.size(): torch.Size([1, 4, 512])\n"
          ]
        }
      ]
    }
  ]
}