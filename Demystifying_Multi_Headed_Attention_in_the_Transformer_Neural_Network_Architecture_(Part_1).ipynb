{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "H56lWHVrJgxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "1FQWszS9JkM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4 #length of the input sentence\n",
        "batch_size = 1 #helps in parallel processing\n",
        "input_dim = 512 #vector dimension of every word that goes into the attention unit\n",
        "d_model = 512  #output of the attention unit for every single word\n",
        " #(i.e. after coming out as a value vector)\n",
        "\n",
        "# Let's randomly sample some input as if the input is coming out adding the input\n",
        "# embedding vectors with the positional encoding.\n",
        "\n",
        "x = torch.randn((batch_size, sequence_length, input_dim))\n"
      ],
      "metadata": {
        "id": "fPZQReSFJvOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#See x and size - for debugging\n",
        "print(x)\n",
        "x.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihmG1AbTV64p",
        "outputId": "613db1ce-eb15-4415-93b5-ab2e9733bceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.2643, -0.4835,  0.1705,  ...,  0.2310,  0.2089, -0.7768],\n",
            "         [ 0.3003, -0.5567, -0.3078,  ..., -0.2815, -0.3583, -1.0123],\n",
            "         [-0.3623,  1.3805,  0.7164,  ..., -0.5174,  1.1290,  0.5393],\n",
            "         [ 1.3606,  0.2813, -0.4229,  ..., -0.4427,  0.8692, -0.4544]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the qkv_layer - that is for each word(token) we have 3 vectors - q, k , v\n",
        "\n",
        "qkv_layer = nn.Linear(input_dim, 3 * d_model)"
      ],
      "metadata": {
        "id": "fualaqCJWHl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the input to the qkv_layer to generate the q, k, v vectors\n",
        "\n",
        "qkv = qkv_layer(x)"
      ],
      "metadata": {
        "id": "5BOJBsD8X1i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should be noted that it is not necessary to split the above vector into 8 attention heads and then pass it through a linear neural network. Instead you can pass the whole 512 length vector to a linear network unit. After that, you can split and carry out QK^T, scaling, softmax and then obtain the value vector. And then concatenate again and apss it through a linear layer\n",
        "\n",
        "IT is not going to amke any difference follwoing the laws of linear transformation."
      ],
      "metadata": {
        "id": "h6n6j18iyozN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See the size of the qkv for the purpose of debugging/understanding\n",
        "# The size will be  1 x 4 x 1536 (batch dimension x 4 words x 512*3) the q,k,v\n",
        "# concatenated\n",
        "\n",
        "qkv.shape"
      ],
      "metadata": {
        "id": "A6v4bJvDYyzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecdc890a-3ca8-4bbb-a4dc-bbd18cea1a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we have 8 attention heads that we are considering\n",
        "# Each head dimension = 512/8\n",
        "# Reshape the qkv matrix to break down the last dimention into a product of the\n",
        "# number of heads into 3 times the head dimension\n",
        "\n",
        "num_heads = 8\n",
        "head_dim = d_model // num_heads\n",
        "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)"
      ],
      "metadata": {
        "id": "Z6R1oH7PeigI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the size of the reshaped qkv\n",
        "# The size is going to be 1 x 8 x 4 x 192\n",
        "# here, 1 is the batch size, 8 attention heads , 4 words, size of each head\n",
        "# 512/8 = 64\n",
        "# multiply with 3 = 192 (each for q,k,v / word)\n",
        "\n",
        "qkv.shape\n"
      ],
      "metadata": {
        "id": "2IBmKHP7Whww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cf4ab3-b734-4aeb-d8cb-80939ca8a622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 8, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qkv = qkv.permute(0, 2, 1, 3)  #\n",
        "# [ batch_size, num_heads, sequence_length, 3*head_dim]\n",
        "# switch the dimensions for ease in parallel processing\n",
        "\n",
        "qkv.shape"
      ],
      "metadata": {
        "id": "93o2u3mZj3BZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdc0e37-ad23-4edb-ab30-0b804c80ac93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the query, key and value vector individually by breaking down the tensor\n",
        "# by its last dimension (hence -1)\n",
        "\n",
        "q, k, v = qkv.chunk(3, dim=-1)\n",
        "q.shape\n",
        "k.shape\n",
        "v.shape\n"
      ],
      "metadata": {
        "id": "fvXCX6WKkTpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1655d2e-508e-4818-f169-b0e2eb8872c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._functorch.vmap import doesnt_support_saved_tensors_hooks\n",
        "#Perform the Attension Mechanism\n",
        "#Just as in Slef Attention\n",
        "#See notebook of Self Attention\n",
        "\n",
        "#Get the size of the one of the vectors of one of the heads\n",
        "\n",
        "d_k = q.size()[-1]\n",
        "\n",
        "#Carry out qk^T and do the scaling in order that variance is smaller.\n",
        "#we want to transpose across last 2 dimensions - sequence length and head\n",
        "#dimensioin size\n",
        "\n",
        "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "scaled.shape"
      ],
      "metadata": {
        "id": "3Yec4-9lktpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac17331-66ea-441e-e4a9-7489b29698d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's talk about masking\n",
        "# In the Encoder, we do not require amsking\n",
        "# In Decoder we do require masking as we do not want to know the relavence of\n",
        "# the next words. We do not want the behaviour to be bi-directional for language\n",
        "# In Decoder we generate words one ata  time.\n",
        "# So while generating context we want only the words before it.\n",
        "# We do not have the words after it!\n",
        "# What will it gain context from?!\n",
        "\n",
        "# Do the masking as follows:\n",
        "\n",
        "# 1) We have the saled tensor. This is the 1 x 8 x 4 x 4\n",
        "# 2) We will fill this up with negative infinity values\n",
        "# 3) We taken an upper triangular matrix wheer we leave the values above the\n",
        "# main diagonal as it is and fill the lowe r diagonal with zero\n",
        "# 4) Add the scaled mass\n",
        "# 5) Apply softmax to the new scaled tensor\n",
        "# 6) Get the value vectors\n",
        "\n",
        "mask = torch.full(scaled.size(), float('-inf')) # step 1 and 2 above\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "mask[0][1] # mask for input to a single head\n",
        "\n"
      ],
      "metadata": {
        "id": "txouyOTNxebB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcd8e6e-f971-44ad-f0df-74877cc3fb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled += mask"
      ],
      "metadata": {
        "id": "Zw8CNp5uyCx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carry out softmax t convert numbers into probabilities\n",
        "#i.e. between 0 and 1\n",
        "#Apply softmax to the last dimension\n",
        "\n",
        "attention = F.softmax(scaled, dim=-1)\n",
        "attention.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAdF2cVxzL54",
        "outputId": "4a02db99-3b5d-4615-94aa-d12013c7303d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We now multiply the attention matrix with bvalue vector\n",
        "#This will give us the final transformed word embeddings which are contextually\n",
        "#rich\n",
        "\n",
        "values = torch.matmul(attention, v)\n",
        "values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7X2RDnlz_8Y",
        "outputId": "af84d7c3-f7b3-4702-fa32-443a0f8abb42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}